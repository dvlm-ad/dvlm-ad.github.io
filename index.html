<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning">
  <meta property="og:title" content="dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning"/>
  <meta property="og:description" content="Diffusion VLM for consistent and controllable end-to-end driving"/>
  <meta property="og:url" content="https://dvlm-ad.github.io"/>
  <meta property="og:image" content="static/images/dvlm_ad_banner.png"/>

  <meta name="twitter:title" content="dVLM-AD: Diffusion Vision-Language Model for Driving">
  <meta name="twitter:description" content="Controllable reasoning and planning via diffusion modeling">
  <meta name="twitter:image" content="static/images/dvlm_ad_twitter_banner.png">

  <title>dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Base font */
    body { font-size: 0.85rem; }

    /* Clear hierarchy */
    h1.title.is-1.publication-title { font-size: 2.3rem; }
    h2.title.is-3.has-text-centered { font-size: 1.8rem; margin-bottom: .6rem; }
    h3.subtitle.is-size-4-tablet    { font-size: 0.85rem; }
    h4.title.is-4.has-text-centered { font-size: 1.3rem; }

    /* Paragraph & code */
     p.is-size-5, .content p {
      font-size: 1.05rem !important; /* force override */
      line-height: 1.45;
      font-family: "Calibri", "Arial", sans-serif !important;
    }
    pre code                { font-size: 0.7rem; }

    /* Layout width */
    .container.is-max-desktop { max-width: 900px; }

    hr.section-divider { margin: .4rem 0 1.2rem 0; border: none; height: 1px; background: #e0e0e0; }
    .results-carousel .item {
    margin: 5px;
    padding: 20px;
    height: auto !important;        /* let item stretch to fit text */
    overflow: visible;              /* ensure text isn’t clipped */
    font-size: initial;             /* re‑enable inherited font size */
  }
  </style>
</head>
<body>

<!-- ────────── HERO TITLE ────────── -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <i>d</i>VLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Yingzi Ma<sup>1</sup></span>,
            <span class="author-block">Yulong Cao<sup>2</sup></span>,
            <span class="author-block">Wenhao Ding<sup>2</sup></span>,
            <span class="author-block">Shuibai Zhang<sup>1</sup></span>,
            <span class="author-block">Yan Wang<sup>2</sup></span>,<br>
            <span class="author-block">Ming Jiang<sup>1</sup></span>,
            <span class="author-block">Boris Ivanovic<sup>2</sup></span>,
            <span class="author-block">Marco Pavone<sup>2</sup></span>,
            <span class="author-block">Chaowei Xiao<sup>2,3</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> University of Wisconsin–Madison</span>,
            <span class="author-block"><sup>2</sup> NVIDIA</span>,
            <span class="author-block"><sup>3</sup> Johns Hopkins University</span>
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <a href="https://dvlm-ad.github.io/"
             target="_blank"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="ai ai-arxiv"></i></span>
            <span>Paper</span>
          </a>
          <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon)</span>
          </a>
          <br><br>
          <figure class="image" style="max-width: 850px; margin: 0 auto 1rem;">
            <img src="static/images/challenge.jpg" alt="Challenges in driving VLMs">
          </figure>
          
          <p class="is-size-5" style="margin-bottom: 0.4rem;">
            <i class="fas fa-exclamation-triangle" style="margin-right: 8px;"></i>
            <b>Challenge 1:</b> Reasoning–Action Inconsistency:  
            <span style="font-weight: normal;">
               the predicted trajectory often contradicts the model’s stated reasoning.
            </span>
          </p>
          
          <p class="is-size-5" style="margin-bottom: 1.2rem;">
            <i class="fas fa-exclamation-triangle" style="margin-right: 8px;"></i>
            <b>Challenge 2:</b> Uncontrollable Generation:  
            <span style="font-weight: normal;">
               structured reasoning can be bypassed or corrupted by prompt-level perturbations.
            </span>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <div class="video-wrapper" style="display: flex; justify-content: center;">
          <div class="video-container"
               style="position: relative; width: 70%; padding-bottom: 39.375%; height: 0;
                      overflow: hidden; border-radius: 12px;
                      box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
            <iframe
              src="https://drive.google.com/file/d/1tY-RV7hQ-YhiZYStUrb1hJlHtJ_2fRj4/preview"
              frameborder="0"
              allow="autoplay; fullscreen"
              allowfullscreen
              style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
            </iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- ────────── ABSTRACT ────────── -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <h2 class="title is-3 has-text-centered">Abstract</h2>
        <hr class="section-divider">
        <p class="is-size-5">
          The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. 
          A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision–language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. 
          However, most existing VLMs or vision–language agents (VLAs) are built upon autoregressive (AR) models.
          <br><br>
          In this paper, we observe that existing AR-based VLMs&mdash;limited by causal attention and sequential token generation&mdash;often fail to maintain consistency and controllability between high-level reasoning and low-level planning. 
          In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. 
          Building on these observations, we introduce <b><i>d</i>VLM-AD</b>, a diffusion-based vision–language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving.
          <br><br>
          Evaluated on nuScenes and WOD-E2E, <b><i>d</i>VLM-AD</b> yields more consistent reasoning–action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, 
          outperforming ARM-based baselines with a 9% improvement in behavior–trajectory consistency and a 6% increase in RFS on long-tail WOD-E2E scenarios. 
          These results suggest a controllable and reliable pathway for scalable end-to-end driving.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ────────── OUR APPROACH ────────── -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Our Approach</h2>
      <hr class="section-divider">

      <!-- Image on top, text below -->
      <div class="item has-text-centered">
        <img src="static/images/framework.jpg" alt="SafeVL Reasoning Pipeline"
             style="max-width:80%;border:1px solid #ccc;border-radius:8px;margin-bottom:1.5rem;">
      </div>

      <div class="content has-text-justified">
        <p class="is-size-5">
          <em><b>SafeVL</b></em> performs structured, interpretable reasoning over real and counterfactual driving
          scenarios to evaluate safety in autonomous driving. The framework first employs a
          <b>Road-Graph Counterfactual Data Generation Engine</b> that perturbs agent actions such as acceleration
          and lane changes to synthesize diverse unsafe outcomes. This process expands the range of rare yet
          critical collision-prone scenarios. Then, an <b>Object-centric Visual Reasoning Framework</b> processes
          each driving video through four stages: (S1) scene understanding, (S2) key object detection, (S3) behavior
          prediction for ego and surrounding agents, and (S4) safety analysis. By aligning safe trajectories with
          their counterfactual unsafe variants under the same reasoning pipeline, SafeVL learns to identify and
          explain high-risk events before collisions occur, producing transparent and reliable Safe/Unsafe decisions.
        </p>
      </div>
    </div>
  </div>
</section>




 






<!-- ────────── BIBTEX ────────── -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <hr class="section-divider">
<pre><code>@inproceedings{vpd_lm_neurips2025,
  title  = {SafeVL: Driving Safety Evaluation via Meticulous Reasoning in Vision Language Models},
  author = {Anonymous Author(s)},
  booktitle = {Submitted to ICRA 2026},
  year   = {2025}
}</code></pre>
  </div>
</section>

<!-- ────────── FOOTER ────────── -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="color:gray;font-size:9.9px;">
            Page built with
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
              Academic Project Page Template
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  /* Optional highlight helper (legacy) */
  function toggleHighlight(tag, color) {
    const tgt = event.target;
    const els = document.querySelectorAll('.' + tag);
    const active = tgt.classList.contains('highlight');
    document.querySelectorAll('.highlight').forEach(e => e.classList.remove('highlight'));
    if (!active) {
      tgt.classList.add('highlight');
      els.forEach(e => e.style.backgroundColor = color);
    } else {
      els.forEach(e => e.style.backgroundColor = '');
    }
  }
</script>
</body>
</html>
