<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning">
  <meta property="og:title" content="dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning"/>
  <meta property="og:description" content="Diffusion VLM for consistent and controllable end-to-end driving"/>
  <meta property="og:url" content="https://dvlm-ad.github.io"/>
  <meta property="og:image" content="static/images/dvlm_ad_banner.png"/>

  <meta name="twitter:title" content="dVLM-AD: Diffusion Vision-Language Model for Driving">
  <meta name="twitter:description" content="Controllable reasoning and planning via diffusion modeling">
  <meta name="twitter:image" content="static/images/dvlm_ad_twitter_banner.png">

  <title>dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Base font */
    body { font-size: 0.85rem; }

    /* Clear hierarchy */
    h1.title.is-1.publication-title { font-size: 2.3rem; }
    h2.title.is-3.has-text-centered { font-size: 1.8rem; margin-bottom: .6rem; }
    h3.subtitle.is-size-4-tablet    { font-size: 0.85rem; }
    h4.title.is-4.has-text-centered { font-size: 1.3rem; }

    /* Smaller fonts in Demonstration carousel */
    #demo-carousel h2.subtitle {
      font-size: 1.1rem;   /* Ê†áÈ¢òÂ∞è‰∏ÄÂè∑ */
    }
    
    #demo-carousel h3.subtitle {
      font-size: 0.95rem;  /* ËØ¥ÊòéÊñáÂ≠óÂÜçÂ∞è‰∏ÄÁÇπ */
    }


    /* Paragraph & code */
     p.is-size-5, .content p {
      font-size: 1.05rem !important; /* force override */
      line-height: 1.45;
      font-family: "Calibri", "Arial", sans-serif !important;
    }
    pre code                { font-size: 0.7rem; }

    /* Layout width */
    .container.is-max-desktop { max-width: 900px; }

    hr.section-divider { margin: .4rem 0 1.2rem 0; border: none; height: 1px; background: #e0e0e0; }
    .results-carousel .item {
    margin: 5px;
    padding: 20px;
    height: auto !important;        /* let item stretch to fit text */
    overflow: visible;              /* ensure text isn‚Äôt clipped */
    font-size: initial;             /* re‚Äëenable inherited font size */
  }
  </style>
</head>
<body>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ HERO TITLE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <i>d</i>VLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Yingzi Ma<sup>1</sup></span>,
            <span class="author-block">Yulong Cao<sup>2</sup></span>,
            <span class="author-block">Wenhao Ding<sup>2</sup></span>,
            <span class="author-block">Shuibai Zhang<sup>1</sup></span>,
            <span class="author-block">Yan Wang<sup>2</sup></span>,<br>
            <span class="author-block">Ming Jiang<sup>1</sup></span>,
            <span class="author-block">Boris Ivanovic<sup>2</sup></span>,
            <span class="author-block">Marco Pavone<sup>2</sup></span>,
            <span class="author-block">Chaowei Xiao<sup>2,3</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> University of Wisconsin‚ÄìMadison</span>,
            <span class="author-block"><sup>2</sup> NVIDIA</span>,
            <span class="author-block"><sup>3</sup> Johns Hopkins University</span>
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <a href="https://dvlm-ad.github.io/"
             target="_blank"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="ai ai-arxiv"></i></span>
            <span>Paper</span>
          </a>
          <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon)</span>
          </a>
          <br><br>
          <figure class="image" style="max-width: 850px; margin: 0 auto 1rem;">
            <img src="static/images/challenge.jpg" alt="Challenges in driving VLMs">
          </figure>
          
          <p class="is-size-5" style="margin-bottom: 0.4rem;">
            <i class="fas fa-exclamation-triangle" style="margin-right: 8px;"></i>
            <b>Challenge 1:</b> Reasoning‚ÄìAction Inconsistency:  
            <span style="font-weight: normal;">
               the predicted trajectory often contradicts the model‚Äôs stated reasoning.
            </span>
          </p>
          
          <p class="is-size-5" style="margin-bottom: 1.2rem;">
            <i class="fas fa-exclamation-triangle" style="margin-right: 8px;"></i>
            <b>Challenge 2:</b> Uncontrollable Generation:  
            <span style="font-weight: normal;">
               structured reasoning can be bypassed or corrupted by prompt-level perturbations.
            </span>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <div class="video-wrapper" style="display: flex; justify-content: center;">
          <div class="video-container"
               style="position: relative; width: 70%; padding-bottom: 39.375%; height: 0;
                      overflow: hidden; border-radius: 12px;
                      box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
            <iframe
              src="https://drive.google.com/file/d/1tY-RV7hQ-YhiZYStUrb1hJlHtJ_2fRj4/preview"
              frameborder="0"
              allow="autoplay; fullscreen"
              allowfullscreen
              style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
            </iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ABSTRACT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <h2 class="title is-3 has-text-centered">Abstract</h2>
        <hr class="section-divider">
        <p class="is-size-5">
          The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. 
          A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision‚Äìlanguage models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. 
          However, most existing VLMs or vision‚Äìlanguage agents (VLAs) are built upon autoregressive (AR) models.
          <br><br>
          In this paper, we observe that existing AR-based VLMs&mdash;limited by causal attention and sequential token generation&mdash;often fail to maintain consistency and controllability between high-level reasoning and low-level planning. 
          In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. 
          Building on these observations, we introduce <b><i>d</i>VLM-AD</b>, a diffusion-based vision‚Äìlanguage model that unifies perception, structured reasoning, and low-level planning for end-to-end driving.
          <br><br>
          Evaluated on nuScenes and WOD-E2E, <b><i>d</i>VLM-AD</b> yields more consistent reasoning‚Äìaction pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, 
          outperforming ARM-based baselines with a 9% improvement in behavior‚Äìtrajectory consistency and a 6% increase in RFS on long-tail WOD-E2E scenarios. 
          These results suggest a controllable and reliable pathway for scalable end-to-end driving.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OUR APPROACH ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Our Approach</h2>
      <hr class="section-divider">

      <!-- Image on top, text below -->
      <div class="item has-text-centered">
        <img src="static/images/framework.jpg" alt="SafeVL Reasoning Pipeline"
             style="max-width:80%;border:1px solid #ccc;border-radius:8px;margin-bottom:1.5rem;">
      </div>

      <div class="content has-text-justified">
        <p class="is-size-5">
          ‚öôÔ∏è <b>Unified Diffusion-based Planner</b><br>
          <b><i>d</i>VLM-AD</b> starts from a structured chain-of-thought template that includes critical objects, causal explanations, future meta behavior, and a sequence of waypoints.  
          Instead of left-to-right decoding, a diffusion denoiser iteratively refines all reasoning and action tokens jointly, conditioned on multi-camera views, navigation commands, and ego state.
        </p>
      
        <p class="is-size-5" style="margin-top: 1rem;">
          üß© <b>Controllable Structured Reasoning</b><br>
          During decoding, only editable slots in the template are masked and updated, so the schema itself enforces safety and format constraints.  
          A dynamic denoise strategy with a special <em>reduce</em> token allows variable-length phrases inside fixed windows, avoiding length-matching bias and preserving semantic consistency between behavior and trajectory.
        </p>
      
        <p class="is-size-5" style="margin-top: 1rem;">
          ü™ú <b>Two Training Stages</b><br>
          Stage I aligns the diffusion backbone to the driving domain using about 145k driving-related QA pairs from existing datasets, grounding perception and prediction in realistic scenes.  
          Stage II supervises structured reasoning‚Äìaction pairs on nuScenes and WOD-E2E (23k + 30k samples), so that object detection, explanations, meta behaviors, and trajectories are learned to stay consistent.
        </p>
      </div>
    </div>
  </div>
</section>



<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DEMONSTRATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="hero is-small" id="demo">
  <div class="hero-body">
    <div class="container is-max-desktop">

      <h2 class="title is-3 has-text-centered">Demonstration</h2>
      <hr class="section-divider">


      <!-- Ëá™Âä®ËΩÆÊí≠Ôºöbulma-carousel -->
      <div id="demo-carousel" class="carousel results-carousel">

        <!-- Demo 1 (WOD-E2E) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Intersection with multiple vehicles in dense fog (WOD-E2E)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Intersection with multiple vehicles in dense fog, where
              <em><b><i>d</i>VLM-AD</b></em> shows more consistent ‚Äúslow down‚Äù behavior
              than VLM-AD.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo1.png"  alt="Intersection with multiple vehicles in dense fog"/>
          </div>
        </div>

        <!-- Demo 2 (WOD-E2E) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Downtown intersection with construction barriers and a red light (WOD-E2E)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Downtown intersection with construction barriers and a red light, where
              <em><b><i>d</i>VLM-AD</b></em>'s stopping behavior at the signal is more
              consistent with its textual reasoning than that of VLM-AD.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo2.png"  alt="Downtown intersection with construction barriers and a red light"/>
          </div>
        </div>

        <!-- Demo 3 (WOD-E2E) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Urban intersection with multiple vehicles and stop signs (WOD-E2E)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Urban intersection with multiple vehicles and stop signs, where
              <em><b><i>d</i>VLM-AD</b></em> more faithfully couples its reasoning about
              stopping and following distance with the generated trajectory.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo3.png"  alt="Urban intersection with multiple vehicles and stop signs"/>
          </div>
        </div>

        <!-- Demo 4 (WOD-E2E) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Night-time scenario with a left-turn navigation command (WOD-E2E)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Night-time scenario with a left-turn navigation command, where
              <em><b><i>d</i>VLM-AD</b></em> executes a left-turn trajectory aligned with
              its reasoning, while VLM-AD behaves more conservatively.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo4.png"  alt="Night-time scenario with a left-turn navigation command"/>
          </div>
        </div>

        <!-- Demo 5 (WOD-E2E) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Intersection with a stop sign at night (WOD-E2E)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Night-time intersection with a clearly visible stop sign, where
              <em><b><i>d</i>VLM-AD</b></em> produces a more stable ‚Äúcome to stop / go straight‚Äù
              trajectory that better matches its reasoning about the traffic element than VLM-AD.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo5.png"  alt="Intersection with a stop sign at night"/>
          </div>
        </div>

        <!-- Demo 6 (WOD-E2E) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Road ahead with stationary vehicles (WOD-E2E)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Road ahead with stationary vehicles, where
              <em><b><i>d</i>VLM-AD</b></em> maintains a smoother ‚Äúslow down / go straight‚Äù
              trajectory that better reflects its reasoning about safe following distance than VLM-AD.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo6.png" alt="Road ahead with stationary vehicles"/>
          </div>
        </div>

        <!-- Demo 7 (nuScenes) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Scenario with oncoming traffic (nuScenes)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Scenario with oncoming traffic, where
              <em><b><i>d</i>VLM-AD</b></em>'s reasoning process is more accurate
              and less hallucinated than the autoregressive baseline.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo8.png"  alt="Scenario with oncoming traffic"/>
          </div>
        </div>

        <!-- Demo 8 (nuScenes) -->
        <div class="item">
          <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
            Driving along an open road (nuScenes)
          </h2>
          <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
            <p>
              Driving along an open road with a speed-hump marking, where
              <em><b><i>d</i>VLM-AD</b></em>'s lane-following trajectory better matches its
              reasoning about the clear road than VLM-AD.
            </p>
          </h3>
          <div id="dialog-container">
            <img src="static/images/demo7.png"  alt="Driving along an open road with a speed hump"/>
          </div>
        </div>

      </div> <!-- /demo-carousel -->

    </div>
  </div>
</section>



 






<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BIBTEX ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <hr class="section-divider">
<pre><code>@inproceedings{vpd_lm_neurips2025,
  title  = {SafeVL: Driving Safety Evaluation via Meticulous Reasoning in Vision Language Models},
  author = {Anonymous Author(s)},
  booktitle = {Submitted to ICRA 2026},
  year   = {2025}
}</code></pre>
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FOOTER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="color:gray;font-size:9.9px;">
            Page built with
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
              Academic Project Page Template
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  /* Optional highlight helper (legacy) */
  function toggleHighlight(tag, color) {
    const tgt = event.target;
    const els = document.querySelectorAll('.' + tag);
    const active = tgt.classList.contains('highlight');
    document.querySelectorAll('.highlight').forEach(e => e.classList.remove('highlight'));
    if (!active) {
      tgt.classList.add('highlight');
      els.forEach(e => e.style.backgroundColor = color);
    } else {
      els.forEach(e => e.style.backgroundColor = '');
    }
  }
</script>
</body>
</html>
